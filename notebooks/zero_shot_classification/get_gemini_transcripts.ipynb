{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"../..\")))\n",
    "\n",
    "from models import gemini\n",
    "\n",
    "importlib.reload(gemini)\n",
    "from models.gemini import GeminiAsyncRequester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 20\n",
    "CONCURRENCY_LIMIT = 5\n",
    "GEMINI_OUTPUT_JSON_PATH = \"../../data/results/gemini_transcripts.json\"\n",
    "GEMINI_METADATA_OUTPUT_JSON_PATH = \"../../data/results/gemini_metadata.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get video paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total videos to process: 3556\n"
     ]
    }
   ],
   "source": [
    "# Get video paths from dataset metadata\n",
    "video_metadata = pd.read_csv(os.getenv(\"VIDEO_METADATA_PATH\"))\n",
    "video_paths = video_metadata[~video_metadata['is_size_outlier']]['video_path'].tolist()\n",
    "print(f\"Total videos to process: {len(video_paths)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gemini's API supports two approaches for video processing:\n",
    "\n",
    "1. **Direct inline content** (< 20MB): Smaller videos can be embedded directly in the `generateContent` request without using the File API.\n",
    "2. **File API upload** (≥ 20MB): Larger videos must first be uploaded via the File API before processing.\n",
    "\n",
    "Since the dataset contains videos exceeding 20MB, videos will be processed in two separate batches based on their file size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Small videos (< 20MB): 2985\n",
      "Large videos (≥ 20MB): 571\n",
      "Total videos to process: 3556\n"
     ]
    }
   ],
   "source": [
    "# Split videos by size threshold (20MB)\n",
    "small_videos = video_metadata[\n",
    "    (~video_metadata['is_size_outlier']) & \n",
    "    (video_metadata['size_mb'] < 20)\n",
    "]['video_path'].tolist()\n",
    "\n",
    "large_videos = video_metadata[\n",
    "    (~video_metadata['is_size_outlier']) & \n",
    "    (video_metadata['size_mb'] >= 20)\n",
    "]['video_path'].tolist()\n",
    "\n",
    "print(f\"Small videos (< {20}MB): {len(small_videos)}\")\n",
    "print(f\"Large videos (≥ {20}MB): {len(large_videos)}\")\n",
    "print(f\"Total videos to process: {len(small_videos) + len(large_videos)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get prompt\n",
    "prompt_path = \"../../data/prompts/video_understanding_gemini.txt\"\n",
    "with open(prompt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    prompt = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the instance\n",
    "gemini_model = GeminiAsyncRequester(\n",
    "    api_key=os.getenv(\"GEMINI_API_KEY\"),\n",
    "    model=\"models/gemini-2.5-pro\",\n",
    "    prompt=prompt,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def call_gemini_small_videos(video_paths_list, batch_size, concurrency_limit, use_url=False):\n",
    "    \"\"\"\n",
    "    Process videos with Gemini in batches.\n",
    "    \n",
    "    Args:\n",
    "        video_paths_list: List of video file paths to process\n",
    "        batch_size: Number of videos to process per batch\n",
    "        use_url: If True, use File API for large videos; if False, use inline content\n",
    "    \"\"\"\n",
    "    total_batches = (len(video_paths_list) + batch_size - 1) // batch_size\n",
    "    \n",
    "    for i in range(0, len(video_paths_list), batch_size):\n",
    "        batch = video_paths_list[i : i + batch_size]\n",
    "        batch_num = i // batch_size + 1\n",
    "        \n",
    "        print(f\"\\nProcessing batch {batch_num}/{total_batches} ({len(batch)} videos)...\")\n",
    "        print(f\"Video paths in this batch: {batch}\")\n",
    "        await gemini_model.run_async(\n",
    "            batch,\n",
    "            GEMINI_OUTPUT_JSON_PATH,\n",
    "            GEMINI_METADATA_OUTPUT_JSON_PATH,\n",
    "            concurrency_limit=concurrency_limit,\n",
    "            overwrite=False,\n",
    "            append=True,\n",
    "            use_url=use_url,\n",
    "        )\n",
    "        \n",
    "        print(f\"Batch {batch_num} completed and results appended.\")\n",
    "    \n",
    "    print(f\"\\nAll batches processed. Results saved in: {GEMINI_OUTPUT_JSON_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run small videos inline in batches\n",
    "await call_gemini_small_videos(small_videos, batch_size=BATCH_SIZE, concurrency_limit= CONCURRENCY_LIMIT, use_url=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def call_gemini_large_videos(video_paths_list, batch_size, concurrency_limit):\n",
    "    \"\"\"\n",
    "    Process large videos with Gemini: upload, process, then delete.\n",
    "    \n",
    "    Args:\n",
    "        video_paths_list: List of video file paths to process\n",
    "        batch_size: Number of videos to upload and process per batch\n",
    "        concurrency_limit: Max concurrent API requests during processing\n",
    "    \"\"\"\n",
    "    total_batches = (len(video_paths_list) + batch_size - 1) // batch_size\n",
    "    \n",
    "    for i in range(0, len(video_paths_list), batch_size):\n",
    "        batch = video_paths_list[i : i + batch_size]\n",
    "        batch_num = i // batch_size + 1\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Batch {batch_num}/{total_batches}: Processing {len(batch)} large videos\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Step 1: Upload all videos in batch\n",
    "        print(f\"\\n[Step 1/{3}] Uploading {len(batch)} videos...\")\n",
    "        uploaded_files = {}\n",
    "        for video_path in batch:\n",
    "            print(str(video_path))\n",
    "            try:\n",
    "                video_file = gemini_model.upload_video(video_path)\n",
    "                uploaded_files[video_path] = video_file\n",
    "                print(f\"Uploaded {os.path.basename(video_path)}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to upload {os.path.basename(video_path)}: {e}\")\n",
    "        \n",
    "        print(f\"Uploaded {len(uploaded_files)}/{len(batch)} videos\")\n",
    "        \n",
    "        # Step 2: Process uploaded videos\n",
    "        if uploaded_files:\n",
    "            print(f\"\\n[Step 2/{3}] Processing {len(uploaded_files)} uploaded videos...\")\n",
    "            \n",
    "            await gemini_model.run_async(\n",
    "                uploaded_files,\n",
    "                GEMINI_OUTPUT_JSON_PATH,\n",
    "                GEMINI_METADATA_OUTPUT_JSON_PATH,\n",
    "                concurrency_limit=concurrency_limit,\n",
    "                overwrite=False,\n",
    "                append=True,\n",
    "                use_url=True,\n",
    "            )\n",
    "            print(f\"Processing complete\")\n",
    "        \n",
    "        print(f\"Batch {batch_num} completed and cleaned up\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"All large video batches processed!\")\n",
    "    print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process large videos (upload -> process -> delete)\n",
    "await call_gemini_large_videos(large_videos, batch_size=BATCH_SIZE, concurrency_limit=CONCURRENCY_LIMIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total processed videos: 309\n",
      "\n",
      "Missing large videos: 262/571\n",
      "Successfully processed: 309/571\n"
     ]
    }
   ],
   "source": [
    "# Load existing results\n",
    "with open(GEMINI_OUTPUT_JSON_PATH, 'r', encoding='utf-8') as f:\n",
    "    existing_results = json.load(f)\n",
    "\n",
    "# Get list of processed video names\n",
    "processed_videos = {result['video_path'] for result in existing_results}\n",
    "print(f\"Total processed videos: {len(processed_videos)}\")\n",
    "\n",
    "# Find missing large videos\n",
    "missing_large_videos = []\n",
    "for video_path in large_videos:\n",
    "    video_name = os.path.basename(video_path)\n",
    "    if video_name not in processed_videos:\n",
    "        missing_large_videos.append(video_path)\n",
    "\n",
    "print(f\"\\nMissing large videos: {len(missing_large_videos)}/{len(large_videos)}\")\n",
    "print(f\"Successfully processed: {len(large_videos) - len(missing_large_videos)}/{len(large_videos)}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
