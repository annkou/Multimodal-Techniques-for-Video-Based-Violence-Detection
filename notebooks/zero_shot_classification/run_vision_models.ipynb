{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"../..\")))\n",
    "\n",
    "from models import clip, xclip\n",
    "from actions import helpers\n",
    "\n",
    "importlib.reload(clip)\n",
    "importlib.reload(xclip)\n",
    "importlib.reload(helpers)\n",
    "from models.clip import CLIPModel\n",
    "from models.xclip import XCLIPVideoClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.0+cu126\n",
      "CUDA available: True\n",
      "CUDA version: 12.6\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# Check CUDA availability\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIP_OUTPUT_JSON_PATH = \"../../data/results/clip_results.json\"\n",
    "XCLIP_OUTPUT_JSON_PATH = \"../../data/results/xclip_results.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get video paths and candidate labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total videos to process: 3556\n"
     ]
    }
   ],
   "source": [
    "# Get video paths from dataset metadata\n",
    "video_metadata = pd.read_csv(os.getenv(\"VIDEO_METADATA_PATH\"))\n",
    "video_paths = video_metadata[~video_metadata['is_size_outlier']]['video_path'].tolist()\n",
    "print(f\"Total videos to process: {len(video_paths)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vision_labels, associated_objects = helpers.extract_vision_data(\n",
    "    os.getenv(\"LABELS_PATH\")\n",
    ")\n",
    "primary_labels = list(vision_labels.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\CG\\Desktop\\Multimodal-Techniques-for-Video-Based-Violence-Detection\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\CG\\.cache\\huggingface\\hub\\models--openai--clip-vit-base-patch32. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Error while downloading from https://huggingface.co/openai/clip-vit-base-patch32/resolve/main/pytorch_model.bin: HTTPSConnectionPool(host='cas-bridge.xethub.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded CLIP model: openai/clip-vit-base-patch32\n"
     ]
    }
   ],
   "source": [
    "# 1. Initialize CLIP model\n",
    "clip_model = CLIPModel(\n",
    "    model_name=\"openai/clip-vit-base-patch32\",  # or \"openai/clip-vit-large-patch14\"\n",
    "    sample_rate=10,  # Extract every 10th frame\n",
    "    batch_size=8,  # Process 8 frames at a time\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 3556 videos\n"
     ]
    }
   ],
   "source": [
    "await clip_model.process_all_videos(\n",
    "    video_paths=video_paths,\n",
    "    candidate_labels=primary_labels,\n",
    "    output_json=CLIP_OUTPUT_JSON_PATH,\n",
    "    overwrite=False,  # Set to True to start fresh\n",
    "    top_k=3,  # Keep top 3 labels per frame\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 3556 videos\n"
     ]
    }
   ],
   "source": [
    "await clip_model.process_all_videos(\n",
    "    video_paths=video_paths,\n",
    "    candidate_labels=associated_objects,\n",
    "    output_json=CLIP_OUTPUT_JSON_PATH,\n",
    "    overwrite=False,  # Set to True to start fresh\n",
    "    top_k=3,  # Keep top 3 labels per frame\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run X-CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded XCLIP model: microsoft/xclip-base-patch16-zero-shot\n"
     ]
    }
   ],
   "source": [
    "# Initialize X-CLIP model\n",
    "xclip_model = XCLIPVideoClassifier(\n",
    "    model_name=\"microsoft/xclip-base-patch16-zero-shot\",  # or \"microsoft/xclip-large-patch14\"\n",
    "    clip_len=32,  # Number of frames per segment\n",
    "    frame_sample_rate=2,  # Sample every 2nd frame\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\CG\\Desktop\\Multimodal-Techniques-for-Video-Based-Violence-Detection\\.venv\\Lib\\site-packages\\transformers\\image_processing_utils.py:42: UserWarning: The following named arguments are not valid for `VideoMAEImageProcessor.preprocess` and were ignored: 'padding'\n",
      "  return self.preprocess(images, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 3556 videos\n"
     ]
    }
   ],
   "source": [
    "await xclip_model.process_all_videos(\n",
    "    video_paths=video_paths,\n",
    "    labels=primary_labels,\n",
    "    output_json=XCLIP_OUTPUT_JSON_PATH,\n",
    "    overwrite=False,  # Set to True to start fresh\n",
    "    top_k=3,  # Keep top 3 labels per segment\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\CG\\Desktop\\Multimodal-Techniques-for-Video-Based-Violence-Detection\\.venv\\Lib\\site-packages\\transformers\\image_processing_utils.py:42: UserWarning: The following named arguments are not valid for `VideoMAEImageProcessor.preprocess` and were ignored: 'padding'\n",
      "  return self.preprocess(images, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 3556 videos\n"
     ]
    }
   ],
   "source": [
    "await xclip_model.process_all_videos(\n",
    "    video_paths=video_paths,\n",
    "    labels=associated_objects,\n",
    "    output_json=XCLIP_OUTPUT_JSON_PATH,\n",
    "    overwrite=False,  # Set to True to start fresh\n",
    "    top_k=3,  # Keep top 3 labels per segment\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
